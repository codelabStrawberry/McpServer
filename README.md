# ollama-rag-mcp

Ollama + ChromaDB + FastAPI ê¸°ë°˜ **MCP(Model Context Protocol) RAG ì„œë²„** ì˜ˆì œ í”„ë¡œì íŠ¸ì…ë‹ˆë‹¤.

ë¡œì»¬ LLM(Ollama)ì„ ì‚¬ìš©í•´ **Chat / RAG ê²€ìƒ‰ / ë¬¸ì„œ ì¶”ê°€**ë¥¼ MCP Tool í˜•íƒœì˜ APIë¡œ ì œê³µí•©ë‹ˆë‹¤.

---

---

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
MCPSERVER/
â”œâ”€ data/
â”‚ â””â”€ docs/
â”‚ â””â”€ chroma_data1.txt # ì˜ˆì œ RAG ë¬¸ì„œ
â”‚
â”œâ”€ mcp_server/
â”‚ â”œâ”€ api/
â”‚ â”‚ â”œâ”€ routes/
â”‚ â”‚ â”‚ â”œâ”€ chat.py # /mcp/tools/chat
â”‚ â”‚ â”‚ â”œâ”€ rag.py # /mcp/tools/rag_chat
â”‚ â”‚ â”‚ â””â”€ docs.py # /mcp/tools/add_doc, add_doc2
â”‚ â”‚ â”œâ”€ init.py
â”‚ â”‚ â”œâ”€ app.py # FastAPI app + lifespan
â”‚ â”‚ â””â”€ schemas.py # Pydantic request models
â”‚ â”‚
â”‚ â”œâ”€ chroma_db.py # ChromaDB client + ê²€ìƒ‰/ì €ì¥ ë¡œì§
â”‚ â”œâ”€ ollama.py # Ollama API wrapper (chat / embedding)
â”‚ â”œâ”€ ollama_client.py # AsyncClient ì „ì—­ ê´€ë¦¬
â”‚ â”œâ”€ rag.py # RAG prompt êµ¬ì„± ë¡œì§
â”‚ â”œâ”€ ingest.py # ë¬¸ì„œ ingest ìœ í‹¸ (ì„ íƒ)
â”‚ â”œâ”€ main.py # uvicorn entrypoint
â”‚ â”‚
â”‚ â”œâ”€ Dockerfile
â”‚ â”œâ”€ Dockerfile_Debug
â”‚ â”œâ”€ entrypoint.sh # ëª¨ë¸ pull + ì„œë²„ ì‹¤í–‰
â”‚ â”œâ”€ entrypoint_debug.sh
â”‚ â””â”€ requirements.txt
â”‚
â”œâ”€ .env
â”œâ”€ .env_sample
â”œâ”€ docker-compose.yml
â”œâ”€ docker-compose_Debug.yml
â””â”€ README.md
```

---

---

## âš™ï¸ í™˜ê²½ ë³€ìˆ˜ (.env)

```env
# -----------------------
# Ollama
# -----------------------
OLLAMA_BASE_URL=http://ollama:11434

# Chat ëª¨ë¸
OLLAMA_CHAT_MODEL=gemma3:1b

# Embedding ëª¨ë¸ (âš ï¸ ë°˜ë“œì‹œ embedding ì „ìš© ëª¨ë¸)
OLLAMA_EMBED_MODEL=nomic-embed-text

# -----------------------
# ChromaDB
# -----------------------
CHROMA_HOST=chroma
CHROMA_PORT=8000
CHROMA_COLLECTION=rag_docs

```

---

## ğŸš€ ì‹¤í–‰ ìˆœì„œ

### 1ï¸âƒ£ Docker ì»¨í…Œì´ë„ˆ ì‹¤í–‰

```bash
docker-compose up --build -d
docker compose build --no-cache
docker-compose up -d
```

Docker ìƒì„± í›„ `Dockerfile`ì—ì„œ `entrypoint.sh`ë¥¼ í˜¸ì¶œí•˜ì—¬ ì•„ë˜ ëª¨ë¸ì„ ìë™ìœ¼ë¡œ pull í•©ë‹ˆë‹¤.

- `gemma3:1b`
- `nomic-embed-text`

ì„¤ì¹˜ ì—¬ë¶€ëŠ” ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ í™•ì¸í•©ë‹ˆë‹¤.

docker-compose stop
stop í–ˆìœ¼ë©´ : docker-compose up -d

docker-compose down

```bash
docker logs ollama
```

ì»¨í…Œì´ë„ˆ ì¬ì‹¤í–‰ ì‹œ:

```bash
docker compose up -d
```

---

### 2ï¸âƒ£ Ollama ëª¨ë¸ ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ (ì„ íƒ)

#### ê¸°ë³¸ LLM ëª¨ë¸

```bash
docker exec -it ollama ollama pull gemma3:1b
```

#### Embedding ëª¨ë¸ (RAG ë¬¸ì„œìš©)

```bash
docker exec -it ollama ollama pull nomic-embed-text
```

---

## ğŸ³ Docker ë””ë²„ê¹… ëª…ë ¹ì–´

```bash
docker-compose build
docker-compose up -d
docker-compose up --build -d
docker compose build --no-cache
```

---

## ğŸ§ª API í…ŒìŠ¤íŠ¸ (Windows CMD ê¸°ì¤€)

> âš ï¸ **Windows CMDì—ì„œëŠ” ë°˜ë“œì‹œ í•œ ì¤„ JSON + ì´ì¤‘ ë”°ì˜´í‘œ escape ì‚¬ìš©**

---

### ğŸ’¬ Chat (LLM ë‹¨ë…)

```cmd
curl -X POST http://localhost:3333/mcp/tools/chat -H "Content-Type: application/json; charset=utf-8" -d "{\"prompt\":\"MCP ì„œë²„ê°€ ë¬´ì—‡ì¸ì§€ ì„¤ëª…í•´ì¤˜\"}"
```

---

### ğŸ“¥ ë¬¸ì„œ ì¶”ê°€ (RAG ì €ì¥)

```cmd
curl -X POST http://localhost:3333/mcp/tools/add_doc -H "Content-Type: application/json; charset=utf-8" -d "{\"id\":\"doc-001\",\"text\":\"MCP ì„œë²„ëŠ” LLMê³¼ ì™¸ë¶€ ë„êµ¬ë¥¼ ì—°ê²°í•˜ëŠ” ì¤‘ê°„ ê³„ì¸µ ì„œë²„ì´ë‹¤.\"}"
```

```cmd
curl -X POST http://localhost:3333/mcp/tools/add_doc2 -H "Content-Type: application/json; charset=utf-8" -d "{\"id\":\"doc-002\",\"text\":\"RAGëŠ” ê²€ìƒ‰ ê¸°ë°˜ìœ¼ë¡œ LLMì˜ í™˜ê°ì„ ì¤„ì´ëŠ” êµ¬ì¡°ì´ë‹¤.\"}"
```

---

### ğŸ” RAG ì§ˆì˜ (ê²€ìƒ‰ + LLM)

```cmd
curl -X POST http://localhost:3333/mcp/tools/rag_chat -H "Content-Type: application/json; charset=utf-8" -d "{\"question\":\"MCP ì„œë²„ êµ¬ì¡°ë¥¼ RAG ê¸°ì¤€ìœ¼ë¡œ ì„¤ëª…í•´ì¤˜\"}"
```

---

## ğŸ§  ë‚´ë¶€ ë™ì‘ íë¦„

```
Client (curl / MCP)
  â””â”€ FastAPI (/mcp/tools/*)
       â”œâ”€ chat            â†’ Ollama LLM ì‘ë‹µ
       â”œâ”€ add_doc         â†’ Embedding â†’ ChromaDB ì €ì¥
       â””â”€ rag_chat
            â”œâ”€ Embedding (nomic-embed-text)
            â”œâ”€ ChromaDB ê²€ìƒ‰
            â””â”€ Ollama LLM ì‘ë‹µ

